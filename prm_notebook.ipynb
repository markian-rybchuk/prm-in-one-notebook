{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_kuJWRYzPpm"
   },
   "source": [
    "cd drive/MyDrive/elk-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qC9eZkWJJnms",
    "outputId": "a6b11350-e2a9-4a81-ea86-3461108d426e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/elk\n",
    "# !pip install -e .\n",
    "# !elk elicit microsoft/phi-2 imdb\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive\n",
    "# !pip install -e .\n",
    "# !elk elicit microsoft/phi-2 imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P6GHA4Qxjx8s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# prompt: pip install the imports\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install peft --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()  # Write a config file\n",
    "# os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9VNeZAlIUjTb"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "567b1bb37f2e4a3a97ce58d1b4057362",
      "4b09c0438abd4100be988a12d85e7379",
      "90395436a3d84ee48b665d823eb40c58",
      "d99ae2db6e4b4332ab91a37854bb55da",
      "8ecd5a8bf7fc4e87a4d06b4ed3cb76e4",
      "267769c9de0040faba98b772b70a3e4c",
      "27fdd19119a14a91b39222c2d6b8902d",
      "753fe37d480d45c18c2549716b068de8",
      "77e8d0559ebc4d529a7f760076170ac8",
      "a691d1789ba548018309476ccfb126c1",
      "3570b39895e74e508783de60a47a92ff"
     ]
    },
    "id": "x79ppAxujD1n",
    "outputId": "ce759946-84be-4a73-fea3-e8c64f10eb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not implemented!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM #, BitsAndBytesConfig\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# import bitsandbytes as bnb\n",
    "import random\n",
    "\n",
    "# model_name = \"t5\"\n",
    "model_name = \"-\"\n",
    "\n",
    "\n",
    "\n",
    "# if you want to cache the model weights somewhere, you can specify that here\n",
    "cache_dir = None\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=64,\n",
    "#     lora_alpha=8,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.00,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "\n",
    "if model_name == \"deberta\":\n",
    "    model_type = \"encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"phiqlora\":\n",
    "    model_type = \"decoder\"\n",
    "    model_id = \"microsoft/phi-2\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=cache_dir)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, config).cuda()\n",
    "elif model_name == \"llemaqlora\":\n",
    "    model_type = \"decoder\"\n",
    "    model_id = \"EleutherAI/llemma_7b\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, cache_dir=cache_dir)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, config).cuda()\n",
    "    \n",
    "# elif model_name == \"phi\":\n",
    "#     model_type = \"decoder\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "#     model.cuda()\n",
    "elif model_name == \"phi\":\n",
    "    model_type = \"decoder\"\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    \n",
    "    # Define the new special token\n",
    "    special_token = \"[SIGNAL]\"\n",
    "    special_tokens_dict = {'additional_special_tokens': [special_token]}\n",
    "    pos_token, neu_token, neg_token = '[POS]', '[NEU]', '[NEG]'\n",
    "\n",
    "    # Add the special token to the tokenizer\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.add_tokens([pos_token, neu_token, neg_token])\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    \n",
    "    # It's important to resize the token embeddings in the model\n",
    "    # This adjusts the model to account for the new token(s)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Assign the pad token if it's not already set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Make sure your model is on the correct device (e.g., GPU)\n",
    "    model.cuda()\n",
    "elif model_name == \"phiq\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir, load_in_4bit=True)\n",
    "    # model.cuda()\n",
    "\n",
    "elif model_name == \"phipeft\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    model = get_peft_model(AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir), lora_config).cuda()\n",
    "    model.cuda()\n",
    "\n",
    "elif model_name == \"mistralpeft\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir=cache_dir)\n",
    "    model = get_peft_model(AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir=cache_dir), lora_config).cuda()\n",
    "\n",
    "elif model_name == \"mistralq\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir=cache_dir, load_in_4bit=True)\n",
    "\n",
    "elif model_name == \"mixtral\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", cache_dir=cache_dir, load_in_4bit=True)\n",
    "    model.cuda()\n",
    "\n",
    "elif model_name == \"load\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir=cache_dir)\n",
    "    model = torch.load('mistral_finetune_own_data.ipynb').cuda()\n",
    "\n",
    "elif model_name == \"llemapeft\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "    model = get_peft_model(AutoModelForCausalLM.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir, load_in_4bit=True), lora_config).cuda()\n",
    "elif model_name == \"llemma\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "    # Define the new special token\n",
    "    special_token = \"[SIGNAL]\"\n",
    "    special_tokens_dict = {'additional_special_tokens': [special_token]}\n",
    "    \n",
    "    # Add the special token to the tokenizer\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir, load_in_4bit=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Assign the pad token if it's not already set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Make sure your model is on the correct device (e.g., GPU)\n",
    "    # model.cuda()\n",
    "else:\n",
    "    print(\"Not implemented!\")\n",
    "\n",
    "# peft = True\n",
    "# if peft:\n",
    "#     model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"labeler\": \"d8aa7923-b970-45e1-9734-e4a7f6c4a7db\",\n",
      "    \"timestamp\": \"2022-07-17T17:03:28.219211\",\n",
      "    \"generation\": null,\n",
      "    \"is_quality_control_question\": false,\n",
      "    \"is_initial_screening_question\": false,\n",
      "    \"question\": {\n",
      "        \"problem\": \"Find the integer $n,$ $-90 < n < 90,$ such that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ.$\",\n",
      "        \"ground_truth_answer\": \"-48\"\n",
      "    },\n",
      "    \"label\": {\n",
      "        \"steps\": [\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"So we have that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So I guess we want to find an integer n such that $\\\\tan n = \\\\tan 312$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So we're looking for an integer $n$ that satisfies two properties: $-90 < n < 90$ and $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So we need to find the integer n between $-90^\\\\circ$ and $90^\\\\circ$ such that $\\\\tan{n^\\\\circ} = \\\\tan{312^\\\\circ}$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So let's start by drawing a right triangle.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"We know that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ.$\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"We know that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 2\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's start by figuring out what $\\\\tan 312^\\\\circ$ is.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right, and we should mention that the function $\\\\tan$ has a period of $180^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. How can we start?\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Well, let's just look at the second property. We know that $\\\\tan n^\\\\circ = \\\\tan (360^\\\\circ - n^\\\\circ)$.\",\n",
      "                        \"rating\": -1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. We know that $\\\\tan n^\\\\circ = \\\\tan (360^\\\\circ - n^\\\\circ)$.\",\n",
      "                        \"rating\": -1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's first find the value of $n$ that satisfies the second property.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's start by solving $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$ for $n$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 1\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"That's true. That means that $\\\\tan (x^\\\\circ) = \\\\tan (x^\\\\circ + 180^\\\\circ)$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So to find the value of $n$ that satisfies the equation $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$, we can add or subtract $180^\\\\circ$ from $312^\\\\circ$ until we find an angle $n^\\\\circ$ that is between $-90^\\\\circ$ and $90^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So we can subtract $180^\\\\circ$ once from $312^\\\\circ$ to get $n^\\\\circ = 132^\\\\circ$, but that doesn't satisfy the inequality.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So let's try subtracting $180^\\\\circ$ again. This gives us $n^\\\\circ = -48^\\\\circ$, which does satisfy the inequality.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So the integer $n$ that we're looking for is $-48$.\\n\\n# Answer\\n\\n-48\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            }\n",
      "        ],\n",
      "        \"total_time\": 179039,\n",
      "        \"finish_reason\": \"solution\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data_point = json.loads(line)\n",
    "            data.append(data_point)\n",
    "    return data\n",
    "\n",
    "file_path = 'phase1_train.jsonl'\n",
    "data = load_jsonl_data(file_path)\n",
    "\n",
    "# # Example: Print the first data point to check\n",
    "# print(json.dumps(data[0], indent=4))\n",
    "first_completion_text = data[0]['label']['steps'][0]['completions'][0]['text']\n",
    "# print(first_completion_text)\n",
    "print(json.dumps(data[7], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So the total number of people taking either English or French is $f + 2f = 25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $f + 2f = 25$ people taking either English or French. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $2f+f=25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n That means that $2f + f$ is equal to 25. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $f + 2f = 25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $2f+f=25$ or $3f=25$. [SIGNAL] \\n ', [1, 1, 1, -1]]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_all_paths_with_ratings(datapoint, limit=50):\n",
    "    question = datapoint['question']['problem']\n",
    "\n",
    "    steps_completions = [step['completions'] for step in datapoint['label']['steps']]\n",
    "\n",
    "\n",
    "    all_paths_combinations = list(product(*steps_completions))\n",
    "    random.shuffle(all_paths_combinations)\n",
    "    if len(all_paths_combinations) > limit:\n",
    "        all_paths_combinations = all_paths_combinations[:limit]\n",
    "\n",
    "    # print('len', len(all_paths_combinations))\n",
    "    all_paths = []\n",
    "    \n",
    "    for path in all_paths_combinations:\n",
    "        # Append ' [SIGNAL] \\n ' to the end of each step's text, including the question for the first step\n",
    "        path_texts = []\n",
    "        path_texts.append(f\"{question} \\n {path[0]['text']} [SIGNAL] \\n \")\n",
    "        if path[0]['rating'] != -1:\n",
    "            for completion in path[1:]:\n",
    "                text = f\"{completion['text']} [SIGNAL] \\n \"\n",
    "                if completion['rating'] == -1:\n",
    "                    path_texts.append(text)\n",
    "                    break\n",
    "                \n",
    "                path_texts.append(text)\n",
    "        concatenated_path_text = ''.join(path_texts)\n",
    "        # print('p', concatenated_path_text)\n",
    "    \n",
    "        # path_ratings = [completion['rating'] for completion in path]\n",
    "        # path_dict = [concatenated_path_text, path_ratings]\n",
    "        # # print('r', path_ratings)\n",
    "        # all_paths.append(path_dict)\n",
    "        path_ratings = []\n",
    "        for completion in path:\n",
    "            rating = completion['rating']\n",
    "            if rating == -1:\n",
    "                path_ratings.append(rating)\n",
    "                break\n",
    "            path_ratings.append(rating)        \n",
    "            # print('r', path_ratings)\n",
    "        path_dict = [concatenated_path_text, path_ratings]\n",
    "        # if path_dict not in all_paths and len(path_dict)%2==0:\n",
    "        if path_dict not in all_paths:\n",
    "            all_paths.append(path_dict)\n",
    "\n",
    "    sampled_paths = all_paths if len(all_paths) <= limit else random.sample(all_paths, limit)\n",
    "\n",
    "    return sampled_paths\n",
    "    # return (all_paths, all_ratings)\n",
    "\n",
    "# Assuming 'data' contains your loaded JSONL data\n",
    "eeer = get_all_paths_with_ratings(data[6])\n",
    "\n",
    "# # Printing each path and its ratings\n",
    "# for path, ratings in zip(eeer[0], eeer[1]):\n",
    "#     print('Path:', path)\n",
    "#     print('Ratings:', ratings)\n",
    "#     print('---')\n",
    "\n",
    "print(len(eeer))\n",
    "print(eeer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_datapoint_within_char_limit(datapoint, char_limit=3000):\n",
    "    if datapoint['label']['finish_reason'] == 'give_up':\n",
    "        return False\n",
    "\n",
    "    total_chars = sum(len(completion['text']) for step in datapoint['label']['steps'] for completion in step['completions'])\n",
    "    # print(total_chars)\n",
    "    return total_chars < char_limit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paths = []\n",
    "# for i in range(949):\n",
    "for i in range(50):\n",
    "\n",
    "    # print('i', i)\n",
    "    if is_datapoint_within_char_limit(data[i]):\n",
    "        paths.extend(get_all_paths_with_ratings(data[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "2\n",
      "['How many seconds are in 7.8 minutes? \\n 7.8 minutes is the same as 7 minutes and 0.8 minutes. [SIGNAL] \\n Right, and since there are 60 seconds in a minute, then there are 60 * 7 = 420 seconds in 7 minutes. [SIGNAL] \\n And since there are 60 seconds in a minute, then there are 60 * 0.8 = 48 seconds in 0.8 minutes. [SIGNAL] \\n So, in total, there are 420 + 48 = 468 seconds in 7.8 minutes. [SIGNAL] \\n Correct.\\n\\n# Answer\\n\\n468 [SIGNAL] \\n ', [1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(paths))\n",
    "# print(len(all_variation_datapoints))\n",
    "print(len(paths[0]))\n",
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "print(len(paths))\n",
    "# print(paths[28])\n",
    "# for i in range(70):\n",
    "#     print(paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import chain\n",
    "import random\n",
    "import torch.distributed as dist\n",
    "\n",
    "# from accelerate import FullyShardedDataParallelPlugin\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(model.dtype)\n",
    "# accelerator = Accelerator(mixed_precision=\"no\")\n",
    "\n",
    "\n",
    "# def setup():\n",
    "#     rank = dist.get_rank()\n",
    "#     world_size = dist.get_world_size()\n",
    "#     dist.init_process_group(\"nccl\")\n",
    "#     torch.cuda.set_device(rank)\n",
    "#     return rank, world_size\n",
    "\n",
    "# print('s', setup())\n",
    "\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "class MathProblemDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_token_len=1024, special_token=\"[SIGNAL]\"):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.special_token = special_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                # text, soln, true_label = data[0], data[2], data[3]\n",
    "\n",
    "        datapoint = self.data[idx]\n",
    "    \n",
    "        # if None not in datapoint[:-1]:\n",
    "        text = datapoint[0]\n",
    "\n",
    "        labels = datapoint[1]  # Labels are an array of numbers that are either -1, 0, or 1\n",
    "        # print(len(labels))\n",
    "        # print('hh', len(labels))\n",
    "\n",
    "        # print('tt', text)\n",
    "        # print('LL', label)\n",
    "        # labels = [1.0 if label is None else label for label in labels]\n",
    "        # print('h', len(labels))\n",
    "        labels = [0 if x is None else x for x in labels]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        # Calculate how much padding is needed for labels to match max_token_len\n",
    "        padding_length = 30 - labels_tensor.size(0)\n",
    "    \n",
    "        padded_labels = F.pad(labels_tensor, (0, padding_length), 'constant', float('nan'))\n",
    "        text_with_special_token = text + \" \" # + \" \" + self.special_token\n",
    "        # print(text_with_special_token)\n",
    "        encoding = self.tokenizer(text_with_special_token, max_length=self.max_token_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        # print(encoding)\n",
    "        return {\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-2.0)  # Use -1.0 or any appropriate padding value\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "\n",
    "class PRM(nn.Module):\n",
    "    def __init__(self, pre_trained_model = None, tokenizer=None):\n",
    "        super(PRM, self).__init__()\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.linear = nn.Linear(50296, 1).cuda()\n",
    "        self.sigmoid = nn.Sigmoid().cuda()\n",
    "    \n",
    "    def forward(self, data, attention_mask=None):\n",
    "        outputs = self.pre_trained_model(data, output_hidden_states=True, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        signal_token_id = self.tokenizer.convert_tokens_to_ids('[SIGNAL]')\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "        # Getting both batch indices and position indices\n",
    "        batch_indices, pos_indices = (data == signal_token_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        # Initialize a container to hold the positions for each batch\n",
    "        batched_positions = [[] for _ in range(batch_size)]\n",
    "    \n",
    "        # Populate the container with positions for each batch\n",
    "        for batch_idx, pos_idx in zip(batch_indices, pos_indices):\n",
    "            batched_positions[batch_idx.item()].append(pos_idx.item())\n",
    "    \n",
    "        # Convert lists to tensors\n",
    "        batched_positions = [torch.tensor(pos_list, device=data.device) if pos_list else torch.tensor([], device=data.device) \n",
    "                             for pos_list in batched_positions]\n",
    "    \n",
    "        # print('batched_positions', batched_positions)\n",
    "                \n",
    "        # signal_positions = (data == signal_token_id).nonzero(as_tuple=True)[1]\n",
    "        # print('sig', signal_positions)\n",
    "\n",
    "                # Initialize a list to store signal logits for each batch\n",
    "        signal_logits = []\n",
    "        max_len = 0  # Variable to hold the maximum length of signal positions across batches\n",
    "        pos_token, neu_token, neg_token = '[POS]', '[NEU]', '[NEG]'\n",
    "\n",
    "        # Extract logits for signal positions in each batch\n",
    "        for i in range(batch_size):\n",
    "            # If there are no positions for this batch (empty tensor), continue to the next batch\n",
    "            if len(batched_positions[i]) == 0:\n",
    "                # Optionally handle empty positions case, e.g., with zeros or skip\n",
    "                continue\n",
    "            \n",
    "            # Index into the logits tensor for the current batch and positions\n",
    "            current_batch_logits = logits[i, batched_positions[i], :]\n",
    "            current_batch_logits = current_batch_logits[:,[self.tokenizer.convert_tokens_to_ids(pos_token), self.tokenizer.convert_tokens_to_ids(neu_token), self.tokenizer.convert_tokens_to_ids(neg_token)]]\n",
    "            # Append to the list\n",
    "            signal_logits.append(current_batch_logits)\n",
    "\n",
    "        # Assuming signal_logits_list is a list of tensors\n",
    "        signal_logits = [self.sigmoid(logits) for logits in signal_logits]\n",
    "        # signal_logits = torch.stack(signal_logits, dim=0)\n",
    "        max_signals = max(logits.shape[0] for logits in signal_logits if logits.numel() > 0)\n",
    "\n",
    "        \n",
    "        # return probs\n",
    "        # return signal_logits\n",
    "        # Step 2: Pad each tensor to have 'max_signals' rows\n",
    "        padded_signal_logits = []\n",
    "        for logits in signal_logits:\n",
    "            pad_size = max_signals - logits.shape[0]\n",
    "            if pad_size > 0:\n",
    "                # Pad the tensor along the first dimension (number of signals)\n",
    "                padded_tensor = F.pad(logits, (0, 0, 0, pad_size), \"constant\", value=-2)  # Pad bottom rows with zeros\n",
    "            else:\n",
    "                padded_tensor = logits\n",
    "            padded_signal_logits.append(padded_tensor)\n",
    "    \n",
    "        # Step 3: Stack the padded tensors\n",
    "        if padded_signal_logits:\n",
    "            stacked_signal_logits = torch.stack(padded_signal_logits, dim=0)\n",
    "        else:\n",
    "            # Handle the case where all tensors might be empty or signal_logits is an empty list\n",
    "            stacked_signal_logits = torch.zeros(0, max_signals, 3, device=data.device)  # Adjust the 3 if different features\n",
    "        \n",
    "        return stacked_signal_logits\n",
    "\n",
    "\n",
    "# e = entries_with_rating_1[:500] + entries_with_rating_minus_1[:500]\n",
    "# random.shuffle(e)\n",
    "# print(all_variation_datapoints)\n",
    "\n",
    "\n",
    "# dataset = MathProblemDataset(paths, tokenizer)\n",
    "# # for i in range(10):\n",
    "# #     print('ll', dataset[i]['labels'])\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)  # Adjust batch_size as needed\n",
    "\n",
    "# # mdel = PRM(model)\n",
    "# mdel = PRM(model) #.to(device)\n",
    "\n",
    "# # mdel.load_state_dict(torch.load('phi_first900_under1700_max20.pth'))\n",
    "# num_gpus = 2\n",
    "# # mdel = torch.nn.parallel.DataParallel(mdel, device_ids=list(range(num_gpus)), dim=0)\n",
    "\n",
    "# mdel.train()\n",
    "\n",
    "# optimizer = AdamW(mdel.parameters(), lr=1e-6)\n",
    "# criterion = nn.BCELoss()\n",
    "# epoch_losses = []\n",
    "\n",
    "# model, optimizer, dataloader = accelerator.prepare(\n",
    "#     model, optimizer, dataloader\n",
    "# )\n",
    "\n",
    "# for epoch in range(2):  # Example: 1 epoch, adjust as necessary\n",
    "#     epoch_loss = 0.0  # Initialize loss for the epoch\n",
    "#     num_batches = 0  # Keep track of the number of batches\n",
    "#     data_loader_with_progress = tqdm(dataloader, desc=f'Epoch {epoch+1}/{30}')\n",
    "#     # Assuming indices 0 and 1 are for normal labels, 2 for -1, and 3 for -2\n",
    "#     label_mappings = torch.tensor([\n",
    "#         [0, 1, 0],  # Index 0: maps to label 0\n",
    "#         [1, 0, 0],  # Index 1: maps to label 1\n",
    "#         [0, 0, 1],  # Index 2: maps to label -1\n",
    "#         [0, 0, 0]   # Index 3: maps to label -2 (padding)\n",
    "#     ], device=accelerator.device)\n",
    "#     padding_index = 3  # Assuming -2 corresponds to index 3\n",
    "\n",
    "#     for batch in data_loader_with_progress:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(accelerator.device)\n",
    "#         labels = batch['labels'].to(accelerator.device)\n",
    "#         # print('labels', labels)\n",
    "\n",
    "#         # Mapping -2 and -1 to their new indices\n",
    "#         labels = torch.where(labels == -2, 3, labels)  # Maps -2 to index 3\n",
    "#         labels = torch.where(labels == -1, 2, labels)  # Maps -1 to index 2\n",
    "\n",
    "#         labels = labels.long()  # Ensure labels are floating-point\n",
    "\n",
    "#         labels_mapped = label_mappings[labels]\n",
    "#         mask = labels != 3  \n",
    "\n",
    "#         # Directly use model's forward pass which now accepts input_ids\n",
    "#         predictions = mdel(input_ids)\n",
    "#         valid_predictions = predictions[mask]\n",
    "#         valid_labels = labels_mapped[mask].float()\n",
    "\n",
    "#         # Calculate loss only on valid data\n",
    "#         loss = criterion(valid_predictions, valid_labels)\n",
    "#         # loss.backward()\n",
    "#         accelerator.backward(loss)\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()  # Accumulate the loss over each batch\n",
    "#         num_batches += 1\n",
    "# #         data_loader_with_progress.set_description(f\"Epoch {epoch+1}/{30}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "#     average_loss = epoch_loss / num_batches  # Calculate average loss for the epoch\n",
    "#     epoch_losses.append(average_loss)  # Append the average loss to the list\n",
    "#     print(f\"Epoch {epoch+1}/{2}, Average Loss: {average_loss:.4f}\")# dataloader = MathProblemDataset(e, tokenizer)\n",
    "\n",
    "# torch.save(mdel.state_dict(), 'phi_2epochs_under4000.pth')\n",
    "\n",
    "\n",
    "# # # Remember to save your model if necessary\n",
    "# torch.save(mdel.state_dict(), 'll_first900_under1000.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n",
    "def init_distributed_mode():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    torch.cuda.set_device(torch.distributed.get_rank())\n",
    "    \n",
    "def training_function():\n",
    "    init_distributed_mode()\n",
    "\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "\n",
    "    # Define the new special token\n",
    "    special_token = \"[SIGNAL]\"\n",
    "    special_tokens_dict = {'additional_special_tokens': [special_token]}\n",
    "    pos_token, neu_token, neg_token = '[POS]', '[NEU]', '[NEG]'\n",
    "\n",
    "    # Add the special token to the tokenizer\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.add_tokens([pos_token, neu_token, neg_token])\n",
    "    \n",
    "    # Load the model\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "\n",
    "    \n",
    "    # It's important to resize the token embeddings in the model\n",
    "    # This adjusts the model to account for the new token(s)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Assign the pad token if it's not already set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    dataset = MathProblemDataset(paths, tokenizer)\n",
    "    # for i in range(10):\n",
    "    #     print('ll', dataset[i]['labels'])\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)  # Adjust batch_size as needed\n",
    "\n",
    "    # mdel = PRM(model)\n",
    "    mdel = PRM(model, tokenizer)\n",
    "    # mdel = accelerator.prepare_model(mdel)\n",
    "\n",
    "\n",
    "    # mdel.load_state_dict(torch.load('phi_first900_under1700_max20.pth'))\n",
    "    num_gpus = 2\n",
    "    # mdel = torch.nn.parallel.DataParallel(mdel, device_ids=list(range(num_gpus)), dim=0)\n",
    "    mdel=mdel.cuda()\n",
    "    mdel = FSDP(mdel)\n",
    "    # mdel.train()\n",
    "\n",
    "    optimizer = AdamW(mdel.parameters(), lr=1e-6)\n",
    "    criterion = nn.BCELoss()\n",
    "    epoch_losses = []\n",
    "\n",
    "\n",
    "    for epoch in range(2):  # Example: 1 epoch, adjust as necessary\n",
    "        epoch_loss = 0.0  # Initialize loss for the epoch\n",
    "        num_batches = 0  # Keep track of the number of batches\n",
    "        data_loader_with_progress = tqdm(dataloader, desc=f'Epoch {epoch+1}/{30}')\n",
    "        # Assuming indices 0 and 1 are for normal labels, 2 for -1, and 3 for -2\n",
    "        label_mappings = torch.tensor([\n",
    "            [0, 1, 0],  # Index 0: maps to label 0\n",
    "            [1, 0, 0],  # Index 1: maps to label 1\n",
    "            [0, 0, 1],  # Index 2: maps to label -1\n",
    "            [0, 0, 0]   # Index 3: maps to label -2 (padding)\n",
    "        ], device=device)\n",
    "        padding_index = 3  # Assuming -2 corresponds to index 3\n",
    "\n",
    "        for batch in data_loader_with_progress:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # print('labels', labels)\n",
    "\n",
    "            # Mapping -2 and -1 to their new indices\n",
    "            labels = torch.where(labels == -2, 3, labels)  # Maps -2 to index 3\n",
    "            labels = torch.where(labels == -1, 2, labels)  # Maps -1 to index 2\n",
    "\n",
    "            labels = labels.long()  # Ensure labels are floating-point\n",
    "\n",
    "            labels_mapped = label_mappings[labels]\n",
    "            mask = labels != 3  \n",
    "\n",
    "            # Directly use model's forward pass which now accepts input_ids\n",
    "            predictions = mdel(input_ids)\n",
    "            valid_predictions = predictions[mask]\n",
    "            valid_labels = labels_mapped[mask].float()\n",
    "\n",
    "            # Calculate loss only on valid data\n",
    "            loss = criterion(valid_predictions, valid_labels)\n",
    "            # loss.backward()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()  # Accumulate the loss over each batch\n",
    "            num_batches += 1\n",
    "            data_loader_with_progress.set_description(f\"Epoch {epoch+1}/{30}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.save(mdel.state_dict(), 'phi_first900_under1700_max20.pth')\n",
    "# mdel = CustomPhiQloraModel(model)\n",
    "# mdel.load_state_dict(torch.load('phi_first900_under1700_max20.pth'))\n",
    "torch.save(mdel.state_dict(), 'phi_phase1_epoch_1_under3000.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_file_path = 'phase1_test.jsonl'\n",
    "test_data = load_jsonl_data(test_file_path)\n",
    "\n",
    "test_variations = []\n",
    "for i in range(5):\n",
    "    if is_datapoint_within_char_limit(test_data[i], char_limit=3000):\n",
    "        test_variations.extend(get_all_paths_with_ratings(test_data[i], limit=10000))\n",
    "        # print(get_all_paths_with_ratings(test_data[i], limit=5))\n",
    "        # print(test_data[i]['question']['problem'])\n",
    "        # print('i', i)\n",
    "\n",
    "print(test_variations[0])\n",
    "# test_variation_datapoints = process_variations(test_variations)\n",
    "\n",
    "# print(mdel.forward(dataloader[0].to(device)))\n",
    "test_dataset = MathProblemDataset(test_variations, tokenizer)\n",
    "# for i in range(10):\n",
    "#     print('ll', dataset[i]['labels'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Adjust batch_size as needed\n",
    "\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "mdel.eval()\n",
    "errors = []\n",
    "for batch in test_dataloader:\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    labels = labels.float()  # Ensure labels are floating-point\n",
    "    # print(labels)\n",
    "    # for label in labels:\n",
    "    #     label = label != -5000\n",
    "    # valid_labels_mask = (labelçs != -5000)\n",
    "    # Apply the mask to filter out -5000 values\n",
    "    # labels = labels[valid_labels_mask]\n",
    "    # print(labels)\n",
    "\n",
    "    # Directly use model's forward pass which now accepts input_ids\n",
    "    with torch.no_grad():\n",
    "        predictions = mdel(input_ids)\n",
    "    # predictions = predictions.clamp(0, 1)\n",
    "    # labels = labels.clamp(0, 1)\n",
    "\n",
    "    # print(predictions.shape, labels.shape)\n",
    "    # print(predictions, labels)\n",
    "    # # print(predictions)\n",
    "    print('preds', predictions.squeeze())\n",
    "    print('labels', labels.squeeze())\n",
    "    print('----')\n",
    "    # print(sum(predictions.squeeze()))\n",
    "    # print(sum(labels.squeeze()))\n",
    "    # print(sum(labels.squeeze()) - sum(predictions.squeeze()))\n",
    "    # errors.append(abs(sum(labels.squeeze()) - sum(predictions.squeeze())))\n",
    "\n",
    "\n",
    "print(sum(errors))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(errors)/len(errors))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80286374679f2ad472c61c83fc267d31329b5dea8e2dcaccb727123767724c5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "267769c9de0040faba98b772b70a3e4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27fdd19119a14a91b39222c2d6b8902d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3570b39895e74e508783de60a47a92ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b09c0438abd4100be988a12d85e7379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_267769c9de0040faba98b772b70a3e4c",
      "placeholder": "​",
      "style": "IPY_MODEL_27fdd19119a14a91b39222c2d6b8902d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "567b1bb37f2e4a3a97ce58d1b4057362": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b09c0438abd4100be988a12d85e7379",
       "IPY_MODEL_90395436a3d84ee48b665d823eb40c58",
       "IPY_MODEL_d99ae2db6e4b4332ab91a37854bb55da"
      ],
      "layout": "IPY_MODEL_8ecd5a8bf7fc4e87a4d06b4ed3cb76e4"
     }
    },
    "753fe37d480d45c18c2549716b068de8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77e8d0559ebc4d529a7f760076170ac8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ecd5a8bf7fc4e87a4d06b4ed3cb76e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90395436a3d84ee48b665d823eb40c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_753fe37d480d45c18c2549716b068de8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77e8d0559ebc4d529a7f760076170ac8",
      "value": 2
     }
    },
    "a691d1789ba548018309476ccfb126c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d99ae2db6e4b4332ab91a37854bb55da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a691d1789ba548018309476ccfb126c1",
      "placeholder": "​",
      "style": "IPY_MODEL_3570b39895e74e508783de60a47a92ff",
      "value": " 2/2 [00:25&lt;00:00, 10.79s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
