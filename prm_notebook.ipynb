{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6GHA4Qxjx8s"
   },
   "outputs": [],
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes --quiet\n",
    "!pip install torch\n",
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install fairscale --quiet\n",
    "!pip install peft --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "567b1bb37f2e4a3a97ce58d1b4057362",
      "4b09c0438abd4100be988a12d85e7379",
      "90395436a3d84ee48b665d823eb40c58",
      "d99ae2db6e4b4332ab91a37854bb55da",
      "8ecd5a8bf7fc4e87a4d06b4ed3cb76e4",
      "267769c9de0040faba98b772b70a3e4c",
      "27fdd19119a14a91b39222c2d6b8902d",
      "753fe37d480d45c18c2549716b068de8",
      "77e8d0559ebc4d529a7f760076170ac8",
      "a691d1789ba548018309476ccfb126c1",
      "3570b39895e74e508783de60a47a92ff"
     ]
    },
    "id": "x79ppAxujD1n",
    "outputId": "ce759946-84be-4a73-fea3-e8c64f10eb26"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "cache_dir = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"labeler\": \"d8aa7923-b970-45e1-9734-e4a7f6c4a7db\",\n",
      "    \"timestamp\": \"2022-07-17T17:03:28.219211\",\n",
      "    \"generation\": null,\n",
      "    \"is_quality_control_question\": false,\n",
      "    \"is_initial_screening_question\": false,\n",
      "    \"question\": {\n",
      "        \"problem\": \"Find the integer $n,$ $-90 < n < 90,$ such that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ.$\",\n",
      "        \"ground_truth_answer\": \"-48\"\n",
      "    },\n",
      "    \"label\": {\n",
      "        \"steps\": [\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"So we have that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So I guess we want to find an integer n such that $\\\\tan n = \\\\tan 312$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So we're looking for an integer $n$ that satisfies two properties: $-90 < n < 90$ and $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So we need to find the integer n between $-90^\\\\circ$ and $90^\\\\circ$ such that $\\\\tan{n^\\\\circ} = \\\\tan{312^\\\\circ}$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"So let's start by drawing a right triangle.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"We know that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ.$\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"We know that $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 2\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's start by figuring out what $\\\\tan 312^\\\\circ$ is.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right, and we should mention that the function $\\\\tan$ has a period of $180^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. How can we start?\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Well, let's just look at the second property. We know that $\\\\tan n^\\\\circ = \\\\tan (360^\\\\circ - n^\\\\circ)$.\",\n",
      "                        \"rating\": -1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. We know that $\\\\tan n^\\\\circ = \\\\tan (360^\\\\circ - n^\\\\circ)$.\",\n",
      "                        \"rating\": -1,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's first find the value of $n$ that satisfies the second property.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    },\n",
      "                    {\n",
      "                        \"text\": \"Right. Let's start by solving $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$ for $n$.\",\n",
      "                        \"rating\": 0,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 1\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"That's true. That means that $\\\\tan (x^\\\\circ) = \\\\tan (x^\\\\circ + 180^\\\\circ)$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So to find the value of $n$ that satisfies the equation $\\\\tan n^\\\\circ = \\\\tan 312^\\\\circ$, we can add or subtract $180^\\\\circ$ from $312^\\\\circ$ until we find an angle $n^\\\\circ$ that is between $-90^\\\\circ$ and $90^\\\\circ$.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So we can subtract $180^\\\\circ$ once from $312^\\\\circ$ to get $n^\\\\circ = 132^\\\\circ$, but that doesn't satisfy the inequality.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So let's try subtracting $180^\\\\circ$ again. This gives us $n^\\\\circ = -48^\\\\circ$, which does satisfy the inequality.\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            },\n",
      "            {\n",
      "                \"completions\": [\n",
      "                    {\n",
      "                        \"text\": \"Right. So the integer $n$ that we're looking for is $-48$.\\n\\n# Answer\\n\\n-48\",\n",
      "                        \"rating\": 1,\n",
      "                        \"flagged\": false\n",
      "                    }\n",
      "                ],\n",
      "                \"human_completion\": null,\n",
      "                \"chosen_completion\": 0\n",
      "            }\n",
      "        ],\n",
      "        \"total_time\": 179039,\n",
      "        \"finish_reason\": \"solution\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data_point = json.loads(line)\n",
    "            data.append(data_point)\n",
    "    return data\n",
    "\n",
    "file_path = 'phase1_train.jsonl'\n",
    "data = load_jsonl_data(file_path)\n",
    "\n",
    "# # Example: Print the first data point to check\n",
    "# print(json.dumps(data[0], indent=4))\n",
    "\n",
    "print(json.dumps(data[7], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So the total number of people taking either English or French is $f + 2f = 25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $f + 2f = 25$ people taking either English or French. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $2f+f=25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n That means that $2f + f$ is equal to 25. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $f + 2f = 25$. [SIGNAL] \\n ', [1, 1, 1, -1]], ['One day Max says to Liz, \"Out of the 25 people taking either English or French, you and I are the only two taking both.\\'\\' Liz, being mathematically inclined, responds by pointing out that there are exactly twice as many people in the English class as there are in the French class. How many people are taking English but not French? \\n I think we should start by letting the number of people taking French be a variable, say $f$. [SIGNAL] \\n And then the number of people taking English is $2f$ because there are twice as many people in the English class as there are in the French class. [SIGNAL] \\n That\\'s right. Now we can use the fact that there are 25 people taking either English or French. [SIGNAL] \\n So $2f+f=25$ or $3f=25$. [SIGNAL] \\n ', [1, 1, 1, -1]]]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_all_paths_with_ratings(datapoint, limit=50):\n",
    "    question = datapoint['question']['problem']\n",
    "\n",
    "    steps_completions = [step['completions'] for step in datapoint['label']['steps']]\n",
    "\n",
    "    all_paths_combinations = list(product(*steps_completions))\n",
    "    random.shuffle(all_paths_combinations)\n",
    "    if len(all_paths_combinations) > limit:\n",
    "        all_paths_combinations = all_paths_combinations[:limit]\n",
    "\n",
    "    # print('len', len(all_paths_combinations))\n",
    "    all_paths = []\n",
    "    \n",
    "    for path in all_paths_combinations:\n",
    "        # Append ' [SIGNAL] \\n ' to the end of each step's text, including the question for the first step\n",
    "        path_texts = []\n",
    "        path_texts.append(f\"{question} \\n {path[0]['text']} [SIGNAL] \\n \")\n",
    "        if path[0]['rating'] != -1:\n",
    "            for completion in path[1:]:\n",
    "                text = f\"{completion['text']} [SIGNAL] \\n \"\n",
    "                if completion['rating'] == -1:\n",
    "                    path_texts.append(text)\n",
    "                    break\n",
    "                \n",
    "                path_texts.append(text)\n",
    "        concatenated_path_text = ''.join(path_texts)\n",
    "\n",
    "        path_ratings = []\n",
    "        for completion in path:\n",
    "            rating = completion['rating']\n",
    "            if rating == -1:\n",
    "                path_ratings.append(rating)\n",
    "                break\n",
    "            path_ratings.append(rating)        \n",
    "        path_dict = [concatenated_path_text, path_ratings]\n",
    "        if path_dict not in all_paths:\n",
    "            all_paths.append(path_dict)\n",
    "\n",
    "    sampled_paths = all_paths if len(all_paths) <= limit else random.sample(all_paths, limit)\n",
    "\n",
    "    return sampled_paths\n",
    "\n",
    "example_datapoint = get_all_paths_with_ratings(data[6])\n",
    "\n",
    "\n",
    "print(len(example_datapoint))\n",
    "print(example_datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_datapoint_within_char_limit(datapoint, char_limit=3000):\n",
    "    if datapoint['label']['finish_reason'] == 'give_up':\n",
    "        return False\n",
    "\n",
    "    total_chars = sum(len(completion['text']) for step in datapoint['label']['steps'] for completion in step['completions'])\n",
    "    # print(total_chars)\n",
    "    return total_chars < char_limit\n",
    "\n",
    "paths = []\n",
    "\n",
    "for example in data:\n",
    "    if is_datapoint_within_char_limit(example):\n",
    "        paths.extend(get_all_paths_with_ratings(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MathProblemDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_token_len=1024, special_token=\"[SIGNAL]\"):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.special_token = special_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        datapoint = self.data[idx]\n",
    "        text = datapoint[0]\n",
    "        labels = datapoint[1] \n",
    "\n",
    "        labels = [0 if x is None else x for x in labels]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        # Calculate how much padding is needed for labels to match max_token_len\n",
    "        padding_length = 30 - labels_tensor.size(0)\n",
    "    \n",
    "        padded_labels = F.pad(labels_tensor, (0, padding_length), 'constant', float('nan'))\n",
    "        text_prepared = text + \" \" \n",
    "\n",
    "        encoding = self.tokenizer(text_prepared, max_length=self.max_token_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'labels': padded_labels\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-2.0)  # Use -1.0 or any appropriate padding value\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "\n",
    "class PRM(nn.Module):\n",
    "    def __init__(self, pre_trained_model = None, tokenizer=None):\n",
    "        super(PRM, self).__init__()\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.linear = nn.Linear(50296, 1).cuda()\n",
    "        self.sigmoid = nn.Sigmoid().cuda()\n",
    "    \n",
    "    def forward(self, data, attention_mask=None):\n",
    "        outputs = self.pre_trained_model(data, output_hidden_states=True, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        signal_token_id = self.tokenizer.convert_tokens_to_ids('[SIGNAL]')\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "        # Getting both batch indices and position indices\n",
    "        batch_indices, pos_indices = (data == signal_token_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        # Initialize a container to hold the positions for each batch\n",
    "        batched_positions = [[] for _ in range(batch_size)]\n",
    "    \n",
    "        # Populate the container with positions for each batch\n",
    "        for batch_idx, pos_idx in zip(batch_indices, pos_indices):\n",
    "            batched_positions[batch_idx.item()].append(pos_idx.item())\n",
    "    \n",
    "        # Convert lists to tensors\n",
    "        batched_positions = [torch.tensor(pos_list, device=data.device) if pos_list else torch.tensor([], device=data.device) \n",
    "                             for pos_list in batched_positions]\n",
    "        signal_logits = []\n",
    "        pos_token, neu_token, neg_token = '[POS]', '[NEU]', '[NEG]'\n",
    "\n",
    "        # Extract logits for signal positions in each batch\n",
    "        for i in range(batch_size):\n",
    "            # If there are no positions for this batch (empty tensor), continue to the next batch\n",
    "            if len(batched_positions[i]) == 0:\n",
    "                # Optionally handle empty positions case, e.g., with zeros or skip\n",
    "                continue\n",
    "            \n",
    "            # Index into the logits tensor for the current batch and positions\n",
    "            current_batch_logits = logits[i, batched_positions[i], :]\n",
    "            current_batch_logits = current_batch_logits[:,[self.tokenizer.convert_tokens_to_ids(pos_token), self.tokenizer.convert_tokens_to_ids(neu_token), self.tokenizer.convert_tokens_to_ids(neg_token)]]\n",
    "            # Append to the list\n",
    "            signal_logits.append(current_batch_logits)\n",
    "\n",
    "        # Assuming signal_logits_list is a list of tensors\n",
    "        signal_logits = [self.sigmoid(logits) for logits in signal_logits]\n",
    "\n",
    "        max_signals = max(logits.shape[0] for logits in signal_logits if logits.numel() > 0)\n",
    "\n",
    "        # Step 2: Pad each tensor to have 'max_signals' rows\n",
    "        padded_signal_logits = []\n",
    "        for logits in signal_logits:\n",
    "            pad_size = max_signals - logits.shape[0]\n",
    "            if pad_size > 0:\n",
    "                # Pad the tensor along the first dimension (number of signals)\n",
    "                padded_tensor = F.pad(logits, (0, 0, 0, pad_size), \"constant\", value=-2)  # Pad bottom rows with zeros\n",
    "            else:\n",
    "                padded_tensor = logits\n",
    "            padded_signal_logits.append(padded_tensor)\n",
    "    \n",
    "        # Step 3: Stack the padded tensors\n",
    "        if padded_signal_logits:\n",
    "            stacked_signal_logits = torch.stack(padded_signal_logits, dim=0)\n",
    "        else:\n",
    "            # Handle the case where all tensors might be empty or signal_logits is an empty list\n",
    "            stacked_signal_logits = torch.zeros(0, max_signals, 3, device=data.device)  # Adjust the 3 if different features\n",
    "        \n",
    "        return stacked_signal_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n",
    "\n",
    "def init_distributed_mode():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    torch.cuda.set_device(torch.distributed.get_rank())\n",
    "    \n",
    "def training_function():\n",
    "    init_distributed_mode()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "\n",
    "    # Define the new special token\n",
    "    special_token = \"[SIGNAL]\"\n",
    "    special_tokens_dict = {'additional_special_tokens': [special_token]}\n",
    "    pos_token, neu_token, neg_token = '[POS]', '[NEU]', '[NEG]'\n",
    "\n",
    "    # Add the special token to the tokenizer\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.add_tokens([pos_token, neu_token, neg_token])\n",
    "    \n",
    "    # Load the model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/llemma_7b\", cache_dir=cache_dir)\n",
    "\n",
    "    # It's important to resize the token embeddings in the model\n",
    "    # This adjusts the model to account for the new token(s)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Assign the pad token if it's not already set\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    dataset = MathProblemDataset(paths, tokenizer)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)  # Adjust batch_size as needed, only use shuffle=False when debugging\n",
    "\n",
    "    prm_model = PRM(base_model, tokenizer)\n",
    "\n",
    "    prm_model=prm_model.cuda()\n",
    "    prm_model = FSDP(prm_model)\n",
    "\n",
    "    optimizer = AdamW(prm_model.parameters(), lr=1e-6)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    label_mappings = torch.tensor([\n",
    "        [0, 1, 0],  # Index 0: maps to label 0\n",
    "        [1, 0, 0],  # Index 1: maps to label 1\n",
    "        [0, 0, 1],  # Index 2: maps to label -1\n",
    "        [0, 0, 0]   # Index 3: maps to label -2 (padding)\n",
    "    ], device=device)\n",
    "\n",
    "    num_epochs = 2\n",
    "    \n",
    "    for epoch in range(num_epochs):  # Example: 1 epoch, adjust as necessary\n",
    "        num_batches = 0  # Keep track of the number of batches\n",
    "        data_loader_with_progress = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        # Assuming indices 0 and 1 are for normal labels, 2 for -1, and 3 for -2\n",
    "\n",
    "        for batch in data_loader_with_progress:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # print('labels', labels)\n",
    "\n",
    "            # Mapping -2 and -1 to their new indices\n",
    "            labels = torch.where(labels == -2, 3, labels)  # Maps -2 to index 3\n",
    "            labels = torch.where(labels == -1, 2, labels)  # Maps -1 to index 2\n",
    "\n",
    "            labels = labels.long() \n",
    "\n",
    "            labels_mapped = label_mappings[labels]\n",
    "            mask = labels != 3  \n",
    "\n",
    "            # Directly use model's forward pass\n",
    "            predictions = prm_model(input_ids)\n",
    "            valid_predictions = predictions[mask]\n",
    "            valid_labels = labels_mapped[mask].float()\n",
    "\n",
    "            # Calculate loss only on valid data\n",
    "            loss = criterion(valid_predictions, valid_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            num_batches += 1\n",
    "            data_loader_with_progress.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "    # Only the main process saves the model\n",
    "    if torch.distributed.get_rank() == 0:\n",
    "        # Remove FSDP wrapper to get the underlying model\n",
    "        full_state_dict = prm_model.state_dict()\n",
    "\n",
    "        # Save the state dict\n",
    "        checkpoint_path = 'model_checkpoint.pth'\n",
    "        torch.save(full_state_dict, checkpoint_path)\n",
    "        print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # Return the checkpoint path (optional)\n",
    "    return checkpoint_path if torch.distributed.get_rank() == 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "# set num_processes to the number of GPUs you have\n",
    "checkpoint_path = notebook_launcher(training_function, num_processes=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80286374679f2ad472c61c83fc267d31329b5dea8e2dcaccb727123767724c5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "267769c9de0040faba98b772b70a3e4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27fdd19119a14a91b39222c2d6b8902d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3570b39895e74e508783de60a47a92ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b09c0438abd4100be988a12d85e7379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_267769c9de0040faba98b772b70a3e4c",
      "placeholder": "​",
      "style": "IPY_MODEL_27fdd19119a14a91b39222c2d6b8902d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "567b1bb37f2e4a3a97ce58d1b4057362": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b09c0438abd4100be988a12d85e7379",
       "IPY_MODEL_90395436a3d84ee48b665d823eb40c58",
       "IPY_MODEL_d99ae2db6e4b4332ab91a37854bb55da"
      ],
      "layout": "IPY_MODEL_8ecd5a8bf7fc4e87a4d06b4ed3cb76e4"
     }
    },
    "753fe37d480d45c18c2549716b068de8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77e8d0559ebc4d529a7f760076170ac8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ecd5a8bf7fc4e87a4d06b4ed3cb76e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90395436a3d84ee48b665d823eb40c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_753fe37d480d45c18c2549716b068de8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77e8d0559ebc4d529a7f760076170ac8",
      "value": 2
     }
    },
    "a691d1789ba548018309476ccfb126c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d99ae2db6e4b4332ab91a37854bb55da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a691d1789ba548018309476ccfb126c1",
      "placeholder": "​",
      "style": "IPY_MODEL_3570b39895e74e508783de60a47a92ff",
      "value": " 2/2 [00:25&lt;00:00, 10.79s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
